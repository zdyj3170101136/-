
#### 虚拟内存

虚拟内存，一般是因为需要使用的内存大于实际物理内存，但只有一部分是实际使用的。

一般空指针为虚拟地址0，不会映射到任何地方。

 cpu通过page table把虚拟的页号映射到实际的页号。

（只有应用层才需要，内核都是直接访问）



页面大小一般为4K。

#### page table

page table包括对虚拟内存的描述，权限和位置长度等。

一般进程申请内存就会分配page table。

但是实际访问的时候才会分配物理内存。

按需分配。



SIGSEGV（访问野指针，导致的内存退出。也就是没和page table关联的信号量）

#### file backed

对应的资源为磁盘上的文件，信息包括文件位置，offset，权限。

第一次访问会page fault中断，将实际内存加载到物理内存中

以这种形式加入到内存的数据都会放入page cache中

#### device backed

映射到物理磁盘的物理地址，比如物理内存被swap out后。

#### anonymous

程序自己的数据段和堆栈空间，由于磁盘上找不到文件，所有称为anonmous。

如果内存吃紧。

file backed可以直接删除。（非脏数据）

ananymap 需要swap out。

#### shared

表示和其他进程共享的数据

#### copy on writr

当写的时候才分配，读不处理。

#### MMU

输入虚拟地址和page table，输出物理地址

#### tlb

tlb保存在cpu的l1cache里头，缓存物理地址到虚拟地址的映射。

https://segmentfault.com/a/1190000008125006#item-6-2

#### huge page

 2MB

https://cloud.tencent.com/developer/article/1485099

1.增加TLB的命中率

2.Page是被锁定在内存中，降低内存交换；

3.锁定内存，降低内存释放与占用产生的性能降低；

4.提高内存的性能，降低CPU负载。



[https://ustack.io/2019-11-21-Linux%E4%B9%8Bhugepages%E5%8E%9F%E7%90%86%E6%A6%82%E5%BF%B5.html](https://ustack.io/2019-11-21-Linux之hugepages原理概念.html)



1. HugePages 会在系统启动时，直接分配并保留对应大小的内存区域
2. HugePages 在开机之后，如果没有管理员的介入，是不会释放和改变的。
3. Not swappable: HugePages 是不会swap.也就是没有page-in/page-out。HugePages一直被pin在内存中
4. Relief of TLB pressure:

在purge TLB的时候，减少了事物条目的加载，提高了性能。使用Hugepages后TLB能覆盖更大的内存地址空间，加快地址转换的时间更少的TLB条目，意味着有更大空间用来记录其他的地址空间.

- No ‘kswapd’ Operations：在Linux下进程“kswapd”是管理swap的，如果是大内存，那pages的数量就非常庞大（例如：50G内存包含1千3百万页表条目），就会耗费惊人的CPU资源。如果使用hugepages，kswapd就不会耗费资源去管理它，可以查看文档361670.1。
- Eliminated page table lookup overhead: 因为hugepage是不swappable的，所有就没有page table lookups。
- Faster overall memory performance: 由于虚拟内存需要两步操作才能实际对应到物理内存地址，因此更少的pages，减轻了page table访问热度，避免了page table热点瓶颈问题。

#### cache

#### buffer cache

缓存块设备id和块的编号到具体数据的映射。

#### page cache

文件id 到对应文件数据的映射。

实际上buffer cache只缓存page cache 不缓存的东西。

比如文件元数据

https://segmentfault.com/a/1190000008125006#item-6-2



所有`Page Cache`里的页面都是`File-backed Pages`，`File-backed Pages`在内存不足的时候可以直接写回对应的硬盘文件里，即`Page-out`。而`Anonymous Pages`在内存不足时就只能写到硬盘上的交换区`Swap`里来释放内存，称之为`Swap-out`。

`Anonymous Pages`与用户进程共存，进程退出则`Anonymous pages`释放，而`Page Cache`即使在进程退出后还可以缓存。



pagecache时系统级别的

http://www.0xffffff.org/2019/07/17/42-linux-memory-monitor/

#### swap

当内存不够用的时候换出。

- 对于服务器，使用swap可能导致苟延残喘一会，几乎中断

- 不实用swap就是删掉号内存的进程然后重启。顶多中断一小会而。

  一般不用swap，

https://segmentfault.com/a/1190000008125116

#### l ru

完全准确的执行lru代价太高。因为每次都得更新数据结构。





当前发生缺页中断时，把最长时间没有使用的页面置换掉。

通常通过一个map和一个链表实现。



实际上不会在缺页的时候才是系统空闲。



因此会定期扫描内存。将内存控制在高低水位。

Linux区分四种不同的页面：不可回收的、可交换的、可同步的、可丢弃的。

页面回收时做一下子判断。

（而我们通过filebacked和annoanymous两个不同的lru链表作出判断/）

- ![截屏2020-07-10 下午5.09.24](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-07-10 下午5.09.24.png)

#### swap cache

如果当页面换出时，先把内容写进磁盘，同时修改内存标志位。

但是内存不l立刻释放。

这样，如果刚被换出就需要立刻访问，就可以从swap cache中找出，减少抖动。

#### active 和 inactive

linux分为file和anmuous两种页面。

其中又包括active和inactive总共四种lru页面。



如果swappiness的值为0，那么内核在启动内存回收时，将完全忽略anonymous pages，这将带来一个好处，就是内核只需要扫描page cache对应的inactive list（LRU_ACTIVE_FILE）就可以了，根本不用扫描anonymous pages对应的inactive list（LRU_ACTIVE_ANON），这样能极大的节约内存回收时花在扫描LRU链表上的时间。



active主要是判断是否是活跃的，referrence用来判断是否是最近访问过。

如果只有一个标志位的话，那么需要在一定时间内清除，需要定时器保证。

由于定时器很少支持，因此使用标志位



链表间移动FIFO。



inactive越长，那么抖动更少。但是soft page fault更多。

因为内存固定。



当前仅仅确保active 不要 超过 inactive。



对于文件来说第一次访问就会被移入到inactibe list。因为文件通常只访问一次。

http://tinylab.org/lwn-495543/

#### page cache

一般回收需要14个页面的cache，达到大小后一起回收。

https://www.cnblogs.com/muahao/p/10109712.html

https://www.jianshu.com/p/7ab51b8a6368

https://zhuanlan.zhihu.com/p/118642121

#### 虚拟地址

分为页号和页偏移。

虚拟页号转换成物理页号的高位。

页偏移是物理页号的低位。

2 《〈 10 akb，2 《〈 201m，2 《〈 30，1g。

一般来说offset是12位，表示一个4kb的页表中的4096字节。



![截屏2020-07-21 下午10.38.12](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-07-21 下午10.38.12.png)

然后其中vpn又分为在cache中的多路相联的offset。

去tlb中查找。

使用tt作为索引找到某一行，tl找到具体的一行。

比如四路相联，就需要两位。

![截屏2020-07-21 下午10.40.38](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-07-21 下午10.40.38.png)

如果page fault了之后就去page table中查找。

![截屏2020-07-21 下午10.43.10](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-07-21 下午10.43.10.png)

#### 所有处理器对内存访问时一样的



![截屏2020-07-21 下午10.50.45](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-07-21 下午10.50.45.png)

<img src="/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-07-21 下午10.51.30.png" alt="截屏2020-07-21 下午10.51.30" style="zoom:200%;" />

numa主要每个cpu分为local内存和远程的内存。

对本地访问更快

https://www.cnblogs.com/loyenwang/p/11523678.html

PFN（page frame number）和mem_map数组index的关系是线性的（有一个固定偏移，如果内存对应的物理地址等于0，那么PFN就是数组index）。

而对于FLATMEM来说，如果其管理的的物理内存本身是连续的还好说，如果不连续的话，那么中间一部分物理地址是没有对应的物理内存的，形象的说就像是一个个洞（hole），这会浪费mem_map数组本身占用的内存空间。

#### 不连续内存模型

我感觉其实就是每个cpu下的物理内存是连续的，但是cpu和cpu之间存在空洞。



![截屏2020-07-21 下午10.54.15](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-07-21 下午10.54.15.png)



然后现在由pfn找到每个cpu的node之下的，再通过弄的本身去查找。

就会比之前的复杂。

通常使用pfc高位bit找到对应的node。和页表管理方式差不多。

https://zhuanlan.zhihu.com/p/68346347

NUMA强调的是memory和processor的位置关系，和内存模型其实是没有关系的，只不过，由于同一node上的memory和processor有更紧密的耦合关系（访问更快），因此需要多个node来管理。

```c
#define __pfn_to_page(pfn)            \ 
({    unsigned long __pfn = (pfn);        \ 
    unsigned long __nid = arch_pfn_to_nid(__pfn);  \ 
    NODE_DATA(__nid)->node_mem_map + arch_local_page_offset(__pfn, __nid);\ 
})
```



#### nuca

非均匀缓存访问模型。

#### 稀疏内存模型

，但是memory hotplug的出现让原来完美的设计变得不完美了，因为即便是一个node中的mem_maps[]也有可能是不连续了。



整个连续的物理地址空间是按照一个section一个section来切断的，每一个section内部，其memory是连续的（即符合flat memory的特点），因此，mem_map的page数组依附于section结构（struct mem_section）而不是node结构了（struct pglist_data）。当然，无论哪一种memory model，都需要处理PFN和page之间的对应关系，只不过sparse memory多了一个section的概念，让转换变成了PFN<--->Section<--->page。



https://cloud.tencent.com/developer/article/1518174



主要切断了和cpu之间的连续

![截屏2020-07-21 下午11.06.21](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-07-21 下午11.06.21.png)

#### 32位地址多少内存

4gb。

1 << 20 为1m，1 《〈30为1gb，1 《〈 32为4gb。



内核空间可以访问所有的 CPU 指令和所有的内存空间、I/O 空间和硬件设备。

用户空间只能访问受限的资源，如果需要特殊权限，可以通过系统调用获取相应的资源。

用户空间允许页面中断，而内核空间则不允许。

内核空间和用户空间是针对线性地址空间的。

x86 CPU中用户空间是 0 - 3G 的地址范围，内核空间是 3G - 4G 的地址范围。x86_64 CPU 用户空间地址范围为0x0000000000000000 – 0x00007fffffffffff，内核地址空间为 0xffff880000000000 - 最大地址。

所有内核进程（线程）共用一个地址空间，而用户进程都有各自的地址空间。

作者：零壹技术栈
链接：https://juejin.im/post/6844903949359644680
来源：掘金
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。



#### dma

磁盘控制器缓冲区到内核缓冲区

![截屏2020-08-10 下午12.58.01](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-08-10 下午12.58.01.png)



![img](https://user-gold-cdn.xitu.io/2019/9/20/16d4e94da80fb4e3?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)![截屏2020-08-10 下午1.30.56](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-08-10 下午1.30.56.png)

#### mmap

- 减少一次cpu拷贝

![截屏2020-08-10 下午1.33.01](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-08-10 下午1.41.00.png)

![截屏2020-08-11 下午2.58.06](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-08-11 下午2.58.06.png)



#### 用户态直接io

![截屏2020-08-10 下午1.45.01](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-08-10 下午1.45.01.png)



#### sendfile

- 减少两次上下文切换，一次cpu拷贝
- 数据不会进入用户缓冲区

![截屏2020-08-10 下午1.49.58](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-08-10 下午1.49.58.png)

```go
// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT

func sendfile(outfd int, infd int, offset *int64, count int) (written int, err error) {
   r0, _, e1 := Syscall6(SYS_SENDFILE, uintptr(outfd), uintptr(infd), uintptr(unsafe.Pointer(offset)), uintptr(count), 0, 0)
   written = int(r0)
   if e1 != 0 {
      err = errnoErr(e1)
   }
   return
}
```



- 0cpu拷贝

![截屏2020-08-10 下午1.50.37](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-08-10 下午1.50.37.png)

sendfile + DMA gather copy 拷贝方式同样存在用户程序不能对数据进行修改的问题，而且本身需要硬件的支持，它只适用于将数据从文件拷贝到 socket 套接字上的传输过程。







![截屏2020-08-10 下午2.00.54](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-08-10 下午2.00.54.png)

RocketMQ Kafka Consumer消费消息过程，使用了零拷贝，零拷贝包含以下两种方式

\1. 使用 mmap + write 方式

RocketMQ 选择了这种方式，mmap+write 方式，因为有小块数据传输的需求，效果会比 sendfile 更好。但是RocketMQ控制mmap映射的内存分配与释放的地方非常复杂，稍有不慎就会出问题。

优点：即使频繁调用，使用小块文件传输，效率也很高。

缺点：不能很好的利用 DMA 方式，会比 sendfile 多消耗 CPU，内存安全性控制复杂，需要避免 JVM Crash问题。

\2. 使用 sendfile 方式

优点：可以利用 DMA 方式，消耗 CPU 较少，大块文件传输效率高，无内存安全性问题。

缺点：小块文件效率低于 mmap 方式，只能是 BIO 方式传输，不能使用 NIO。

sendfile：FileChannel.transferTo()只有源为FileChannel才支持transfer这种高效的复制方式，其他如SocketChannel都不支持transfer模式。当然，目的Channel没有这种限制。所以一般可以做FileChannel->FileChannel和FileChannel->SocketChannel的transfer。

KAFKA的索引文件使用mmap+write 方式，data文件使用sendfile 。

下面这个问题就是RocketMQ使用mmap后的潜在问题：

![截屏2020-08-10 下午2.01.58](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-08-10 下午2.01.58.png)

https://cloud.tencent.com/developer/news/333695

#### splice系统调用

splice 系统调用可以在内核空间的读缓冲区（read buffer）和网络缓冲区（socket buffer）之间建立管道（pipeline），从而避免了两者之间的 CPU 拷贝操作。![截屏2020-08-10 下午2.05.01](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-08-10 下午2.05.01.png)

![截屏2020-08-10 下午2.05.15](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-08-10 下午2.05.15.png)

#### go不使用mmap

![image-20200810145852475](/Users/jieyang/Library/Application Support/typora-user-images/image-20200810145852475.png)



https://juejin.im/post/6844903949359644680#heading-17