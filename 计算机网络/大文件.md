#### 大文件

#### 分而治之

一个10g文件，内存1m。找出出现次数最多的k个。

- 1.分而治之/hash映射
  - 顺序读取文件，对于每个词x，取hash(x)%5000，然后把该值存到5000个小文件（记为x0,x1,...x4999）中。这样每个文件大概是200k左右。当然，如果其中有的小文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。
- 2.hash_map统计
  - 对每个小文件，采用trie树/hash_map等统计每个文件中出现的词以及相应的频率。
- 3.堆/归并排序
  - 取出出现频率最大的100个词（可以用含100个结点的最小堆）后，再把100个词及相应的频率存入文件，这样又得到了5000个文件。最后就是把这5000个文件进行归并（类似于归并排序）的过程了。

https://wizardforcel.gitbooks.io/the-art-of-programming-by-july/content/06.02.html



所谓外排序，顾名思义，即是在内存外面的排序，因为当要处理的数据量很大，而不能一次装入内存时，此时只能放在读写较慢的外存储器（通常是硬盘）上。

外排序通常采用的是一种“排序-归并”的策略。

- 在排序阶段，先读入能放在内存中的数据量，将其排序输出到一个临时文件，依此进行，将待排序数据组织为多个有序的临时文件；
- 尔后在归并阶段将这些临时文件组合为一个大的有序文件，也即排序结果。

假定现在有20个数据的文件A：{5 11 0 18 4 14 9 7 6 8 12 17 16 13 19 10 2 1 3 15}，但一次只能使用仅装4个数据的内容，所以，我们可以每趟对4个数据进行排序，即5路归并，具体方法如下述步骤：

- 我们先把“大”文件A，分割为a1，a2，a3，a4，a5等5个小文件，每个小文件4个数据
  - a1文件为：5 11 0 18
  - a2文件为：4 14 9 7
  - a3文件为：6 8 12 17
  - a4文件为：16 13 19 10
  - a5文件为：2 1 3 15
- 然后依次对5个小文件分别进行排序
  - a1文件完成排序后：0 5 11 18
  - a2文件完成排序后：4 7 9 14
  - a3文件完成排序后：6 8 12 17
  - a4文件完成排序后：10 13 16 19
  - a5文件完成排序后：1 2 3 15
- 最终多路归并，完成整个排序



**原题**：搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。假设目前有一千万个记录，这些查询串的重复读比较高，虽然总数是1千万，但是如果去除重复和，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就越热门。请你统计最热门的10个查询串，要求使用的内存不能超过1G。

**提示**：利用trie树，关键字域存该查询串出现的次数，没有出现为0。最后用10个元素的最小推来对出现频率进行排序。



#### bitmap

**、在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数**

**解法一**：采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32 * 2 bit=1 GB内存，还可以接受。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。

**解法二**：也可采用与第1题类似的方法，进行划分小文件的方法。然后在小文件中找出不重复的整数，并排序。然后再进行归并，注意去除重复的元素。”

**2、给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？**

**解法一**：可以用位图/Bitmap的方法，申请512M的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。

**1、文档检索系统，查询那些文件包含了某单词，比如常见的学术论文的关键字搜索**

**提示**：建倒排索引。

// trie

**1、一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析**

**提示**：用trie树统计每个词出现的次数，时间复杂度是O(n*le)（le表示单词的平均长度），然后是找出出现最频繁的前10个词。当然，也可以用堆来实现，时间复杂度是O(n*lg10)。所以总的时间复杂度，是O(n*le)与O(n*lg10)中较大的哪一个。

**2、寻找热门查询**

**原题**：搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。假设目前有一千万个记录，这些查询串的重复读比较高，虽然总数是1千万，但是如果去除重复和，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就越热门。请你统计最热门的10个查询串，要求使用的内存不能超过1G。

**提示**：利用trie树，关键字域存该查询串出现的次数，没有出现为0。最后用10个元素的最小推来对出现频率进行排序。

[
](https://wizardforcel.gitbooks.io/the-art-of-programming-by-july/content/06.10.html)



**给10^7个数据量的磁盘文件排序**

输入：给定一个文件，里面最多含有n个不重复的正整数（也就是说可能含有少于n个不重复正整数），且其中每个数都小于等于n，n=10^7。 输出：得到按从小到大升序排列的包含所有输入的整数的列表。 条件：最多有大约1MB的内存空间可用，但磁盘空间足够。且要求运行时间在5分钟以下，10秒为最佳结果。

**解法一**：位图方案

你可能会想到把磁盘文件进行归并排序，但题目要求你只有1MB的内存空间可用，所以，归并排序这个方法不行。

熟悉位图的朋友可能会想到用位图来表示这个文件集合。例如正如编程珠玑一书上所述，用一个20位长的字符串来表示一个所有元素都小于20的简单的非负整数集合，边框用如下字符串来表示集合{1,2,3,5,8,13}：

```
0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0
```

上述集合中各数对应的位置则置1，没有对应的数的位置则置0。

参考编程珠玑一书上的位图方案，针对我们的10^7个数据量的磁盘文件排序问题，我们可以这么考虑，由于每个7位十进制整数表示一个小于1000万的整数。我们可以使用一个具有1000万个位的字符串来表示这个文件，其中，当且仅当整数i在文件中存在时，第i位为1。采取这个位图的方案是因为我们面对的这个问题的特殊性：

1. 输入数据限制在相对较小的范围内，
2. 数据没有重复，
3. 其中的每条记录都是单一的整数，没有任何其它与之关联的数据。

所以，此问题用位图的方案分为以下三步进行解决：

- 第一步，将所有的位都置为0，从而将集合初始化为空。
- 第二步，通过读入文件中的每个整数来建立集合，将每个对应的位都置为1。
- 第三步，检验每一位，如果该位为1，就输出对应的整数。

经过以上三步后，产生有序的输出文件。令n为位图向量中的位数（本例中为1000 0000），程序可以用伪代码表示如下：

```
//磁盘文件排序位图方案的伪代码  
//copyright@ Jon Bentley  
//July、updated，2011.05.29。  

//第一步，将所有的位都初始化为0  
for i ={0,....n}      
   bit[i]=0;  
//第二步，通过读入文件中的每个整数来建立集合，将每个对应的位都置为1。  
for each i in the input file     
   bit[i]=1;  

//第三步，检验每一位，如果该位为1，就输出对应的整数。  
for i={0...n}      
  if bit[i]==1        
    write i on the output file  
```

上述的位图方案，共需要扫描输入数据两次，具体执行步骤如下：

第一次，只处理1—4999999之间的数据，这些数都是小于5000000的，对这些数进行位图排序，只需要约5000000/8=625000Byte，也就是0.625M，排序后输出。 第二次，扫描输入文件时，只处理4999999-10000000的数据项，也只需要0.625M（可以使用第一次处理申请的内存）。 因此，总共也只需要0.625M 位图的的方法有必要强调一下，就是位图的适用范围为针对不重复的数据进行排序，若数据有重复，位图方案就不适用了。

不过很快，我们就将意识到，用此位图方法，严格说来还是不太行，空间消耗10^7/8还是大于1M（1M=1024*1024空间，小于10^7/8）。

既然如果用位图方案的话，我们需要约1.25MB（若每条记录是8位的正整数的话，则10000000/(1024*1024*8) ~= 1.2M）的空间，而现在只有1MB的可用存储空间，那么究竟该作何处理呢？

**解法二**：多路归并

诚然，在面对本题时，还可以通过计算分析出可以用如2的位图法解决，但实际上，很多的时候，我们都面临着这样一个问题，文件太大，无法一次性放入内存中计算处理，那这个时候咋办呢？分而治之，大而化小，也就是把整个大文件分为若干大小的几块，然后分别对每一块进行排序，最后完成整个过程的排序。k趟算法可以在kn的时间开销内和n/k的空间开销内完成对最多n个小于n的无重复正整数的排序。

比如可分为2块（k=2，1趟反正占用的内存只有1.25/2M），1~4999999，和5000000~9999999。先遍历一趟，首先排序处理1~4999999之间的整数（用5000000/8=625000个字的存储空间来排序0~4999999之间的整数），然后再第二趟，对5000001~1000000之间的整数进行排序处理。

https://wizardforcel.gitbooks.io/the-art-of-programming-by-july/content/06.04.html



#### k-路归并排序

k路需要比较k - 1次。

使用败者树。



1g的文件内存1m

- 首先对1g的文件分块10个文件内存中排序后输出
- 然后使用多路归并排序对每五个文件进行多路归并排序降低读取外村的次数![截屏2020-07-20 下午5.53.56](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-07-20 下午5.53.56.png)

#### 5-路归并排序

![截屏2020-07-20 下午5.55.12](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-07-20 下午5.55.12.png)

把这五个文件的第一个值读到内存里头，然后进行构建一颗败者树，败者树指的是中间节点存储的是失败（也就是比较中大于的文件下标）。

而根节点存储胜者。



然后我们不断的把根节点的值输出到外存。

当最终胜者判断完成后，只需要更新叶子结点 b3 的值，即导入关键字 15，然后让该结点不断同其双亲结点所表示的关键字进行比较，败者留在双亲结点中，胜者继续向上比较。



在败者树中，用父结点记录其左右子结点进行比赛的败者，让胜者参加下一轮的比赛。败者树的根结点记录的是败者，因此，需要加一个结点来记录比赛的最终胜者。

胜者是更小的元素。

![截屏2020-07-20 下午5.59.09](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-07-20 下午5.59.09.png)

![截屏2020-07-20 下午6.00.00](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-07-20 下午6.00.00.png)

![截屏2020-07-20 下午6.00.16](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-07-20 下午6.00.16.png)

https://www.zhihu.com/question/35144290

对照图片，b3与b4比较，失败后跟新为3；

与b0比较，失败后更新为2。

比赛沿着到根结点的[路径](https://baike.baidu.com/item/路径)不断进行，直到ls[1]处。把败者存放在结点ls[1]中，胜者存放在ls[0]中。

（因此最后节点ls[1]中）

什么是败者（比较小的元素）



为什么根节点保存胜者。



为了防止在归并过程中某个归并段变为空，处理的办法为：可以在每个归并段最后附加一个关键字为最大值的记录。这样当某一时刻选出的冠军为最大值时，表明 5 个归并段已全部归并完成。（因为只要还有记录，最终的胜者就不可能是附加的最大值）。



![截屏2020-07-20 下午6.23.35](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-07-20 下午6.23.35.png)