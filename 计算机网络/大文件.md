#### 大文件

#### 分而治之

一个10g文件，内存1m。找出出现次数最多的k个。

- 1.分而治之/hash映射
  - 顺序读取文件，对于每个词x，取hash(x)%5000，然后把该值存到5000个小文件（记为x0,x1,...x4999）中。这样每个文件大概是200k左右。当然，如果其中有的小文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。
- 2.hash_map统计
  - 对每个小文件，采用trie树/hash_map等统计每个文件中出现的词以及相应的频率。
- 3.堆/归并排序
  - 取出出现频率最大的100个词（可以用含100个结点的最小堆）后，再把100个词及相应的频率存入文件，这样又得到了5000个文件。最后就是把这5000个文件进行归并（类似于归并排序）的过程了。

https://wizardforcel.gitbooks.io/the-art-of-programming-by-july/content/06.02.html



所谓外排序，顾名思义，即是在内存外面的排序，因为当要处理的数据量很大，而不能一次装入内存时，此时只能放在读写较慢的外存储器（通常是硬盘）上。

外排序通常采用的是一种“排序-归并”的策略。

- 在排序阶段，先读入能放在内存中的数据量，将其排序输出到一个临时文件，依此进行，将待排序数据组织为多个有序的临时文件；
- 尔后在归并阶段将这些临时文件组合为一个大的有序文件，也即排序结果。

假定现在有20个数据的文件A：{5 11 0 18 4 14 9 7 6 8 12 17 16 13 19 10 2 1 3 15}，但一次只能使用仅装4个数据的内容，所以，我们可以每趟对4个数据进行排序，即5路归并，具体方法如下述步骤：

- 我们先把“大”文件A，分割为a1，a2，a3，a4，a5等5个小文件，每个小文件4个数据
  - a1文件为：5 11 0 18
  - a2文件为：4 14 9 7
  - a3文件为：6 8 12 17
  - a4文件为：16 13 19 10
  - a5文件为：2 1 3 15
- 然后依次对5个小文件分别进行排序
  - a1文件完成排序后：0 5 11 18
  - a2文件完成排序后：4 7 9 14
  - a3文件完成排序后：6 8 12 17
  - a4文件完成排序后：10 13 16 19
  - a5文件完成排序后：1 2 3 15
- 最终多路归并，完成整个排序



**原题**：搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。假设目前有一千万个记录，这些查询串的重复读比较高，虽然总数是1千万，但是如果去除重复和，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就越热门。请你统计最热门的10个查询串，要求使用的内存不能超过1G。

**提示**：利用trie树，关键字域存该查询串出现的次数，没有出现为0。最后用10个元素的最小推来对出现频率进行排序。



**1、文档检索系统，查询那些文件包含了某单词，比如常见的学术论文的关键字搜索**

**提示**：建倒排索引。