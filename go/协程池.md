#### 协程池

使用workerPool实现协程池。



对于每个tcp连接，传进去workerPool的Serve函数。



而在serve中，仅仅是把这个连接传给workerChan里头的chan。



type workerPool struct {
	// Function for serving server connections.
	// It must leave c unclosed.
	WorkerFunc ServeHandler

	MaxWorkersCount int
	
	MaxIdleWorkerDuration time.Duration
	
	Logger Logger
	
	lock         sync.Mutex
	workersCount int
	mustStop     bool
	
	ready []*workerChan
	
	stopCh chan struct{}
	
	workerChanPool sync.Pool
	
	connState func(net.Conn, ConnState)
}

type workerChan struct {
	lastUseTime time.Time
	ch          chan net.Conn
}

- 然后workerChan本身使用内存分配的。
- 我们可以看到对每个workerChan自如其名，传递worker也就是conn，

其中开启一个协程序把workerChan传进workerfunc处理其所处理的

处理完一个就会把这个ch放到ready链表里头去。

然后自然而然没有数据就会阻塞。

- 处理完之后把workerChan放回去



- workerfunc就取得workerChan里头的连接，对每个连接调用Workerfunc，调用完后再把workerchan给放到空闲链表中。



- 当然放进workerChan空闲链表的和放进workerChanPool的是值复制的同一个workerChan



- 这样有两个好处，第一吧，可以有一个workerChanPool，不用每一次都创建，workerChan本身ok
- 第二个，就是workerChan一直在ready链表里头，这样的话，就可以同时一个协程处理多个连接（）



对ready链表的读取FILO，很显然，越近被放进去的，越容易协程序还在运行。

func (wp *workerPool) getCh() *workerChan {
	var ch *workerChan
	createWorker := false

	wp.lock.Lock()
	ready := wp.ready
	n := len(ready) - 1
	if n < 0 {
		if wp.workersCount < wp.MaxWorkersCount {
			createWorker = true
			wp.workersCount++
		}
	} else {
		ch = ready[n]
		ready[n] = nil
		wp.ready = ready[:n]
	}
	wp.lock.Unlock()
	
	if ch == nil {
		if !createWorker {
			return nil
		}
		vch := wp.workerChanPool.Get()
		if vch == nil {
			vch = &workerChan{
				ch: make(chan net.Conn, workerChanCap),
			}
		}
		ch = vch.(*workerChan)
		go func() {
			wp.workerFunc(ch)
			wp.workerChanPool.Put(vch)
		}()
	}
	return ch
}
————————————————
版权声明：本文为CSDN博主「惜暮」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/u010853261/java/article/details/90553975

#### 优雅关闭
单生产者，多消费者的时候。

生产者关闭UI进行了。



https://my.oschina.net/u/4327542/blog/3373426/print

防止数据没有全部处理完。

首先

多生产者的时候，



如果要实现同步关闭，可以搞一个chan = make(chan, 0)传递chan，接受端搞完后关闭这个chan。

发送方等待这个chan被关闭。

```
func (w *Producer) sendCommand(cmd *Command) error {
   doneChan := make(chan *ProducerTransaction)
   err := w.sendCommandAsync(cmd, doneChan, nil)
   if err != nil {
      close(doneChan)
      return err
   }
   t := <-doneChan
   return t.Error
}
```





如何防止一个chan中的数据还没有消费完。

生产者通过closechan，关闭。

而接收者这边，还需要关闭的接受端之后，还需要调用一个cleanup函数用来把值读取完。

```
func (w *Producer) router() {
   for {
      select {
      case t := <-w.transactionChan:
         w.transactions = append(w.transactions, t)
         err := w.conn.WriteCommand(t.cmd)
         if err != nil {
            w.log(LogLevelError, "(%s) sending command - %s", w.conn.String(), err)
            w.close()
         }
      case data := <-w.responseChan:
         w.popTransaction(FrameTypeResponse, data)
      case data := <-w.errorChan:
         w.popTransaction(FrameTypeError, data)
      case <-w.closeChan:
         goto exit
      case <-w.exitChan:
         goto exit
      }
   }

exit:
   w.transactionCleanup()
   w.wg.Done()
   w.log(LogLevelInfo, "exiting router")
}
```

```
func (w *Producer) transactionCleanup() {
   // clean up transactions we can easily account for
   for _, t := range w.transactions {
      t.Error = ErrNotConnected
      t.finish()
   }
   w.transactions = w.transactions[:0]

   // spin and free up any writes that might have raced
   // with the cleanup process (blocked on writing
   // to transactionChan)
   for {
      select {
      case t := <-w.transactionChan:
         t.Error = ErrNotConnected
         t.finish()
      default:
         // keep spinning until there are 0 concurrent producers
         if atomic.LoadInt32(&w.concurrentProducers) == 0 {
            return
         }
         // give the runtime a chance to schedule other racing goroutines
         time.Sleep(5 * time.Millisecond)
      }
   }
}
```

如题会阻塞在nil的chan上，因此我们应该有一个default函数，表示nil的chan的情况。

然后这个transactionchan本身是不关闭的（如果关闭，就会不断返回零值）。。



多生产者多消费者模型。

生产者往datachan中发送数据；消费者接受。

其中这两种都阻塞在stopchan之上。

如果生产者或者一个消费者想要关闭。

可以调用sync.once关闭stopchan，然后所有的都会关闭了。

https://levonfly.github.io/p/8b210700.html

