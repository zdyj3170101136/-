

#### 删除

（1）日志与事务1.delete语句执行删除的过程是每次从表中删除一行，并且同时将该行的删除操作作为事务记录在日志中保存以便进行进行回滚操作。

 2.truncate table则一次性地从表中删除所有的数据并不把单独的删除操作记录记入日志保存，**删除行是不能恢复的**。并且在删除的过程中不会激活与表有关的删除触发器。**执行速度快**。

（2）表和索引所占空间。

  当表被truncate 后，这个表和索引所占用的空间会恢复到初始大小，

  delete操作不会减少表或索引所占用的空间。

  drop 语句将表所占用的空间全释放掉。

（3）一般而言，drop > truncate > delete

（4）应用范围。

  truncate 只能对table；delete可以是table和view

（5）truncate 和delete只删除数据,drop 则删除整个表（结构和数据）。

https://blog.csdn.net/shadow_zed/article/details/78252494



- delete不能对同一张表先select然后再删除

https://blog.csdn.net/qq_29672495/article/details/72668008

#### 内连接，外连接

内：join on,会返回一个笛卡尔积。

外：不仅会返回一个笛卡尔积，而且还返回左表，右边，或者两边都不符合条件的行，虽然行其他部分为空（不是null）

https://blog.csdn.net/qq_38023253/article/details/81635310

#### int

mysql中int表示数字，占用固定的四个字节大小。

而int（3），int（11）指的是显示的大小，如果比如说如果是1.

左边的会显示001.

#### Avg, min, max sum, count

用来统计。

使用distinct来去除重复。

select count（distinct ID）



group by dept_name；

所有相同属性的将会被分在一起作为一组。

每个分组的avg等会被分别计算。



count之外的忽略null

#### 优先级



- from（from默认产生的笛卡尔积，而natural join默认会在一个相同共有的列，对所有相同的连接）

- on对上一步的进行筛选

- join对表进行计算，只找出符合条件的表

- where

- group by形成虚拟的元组

- avg， sum

- having（应用在子句上）用于元组

- select 将制定的列插入到虚拟表中

- distinct 进行去重

- order by排序

- limit跳过前几个, offset只输出几个

  

#### 字符串

mysql字符串大小写不敏感。

'%sss'匹配以sss结尾

'_ _ _'匹配只含三个的字符串

#### 大小写

linux区分大小写，windows不区分。

因此在定义数据库，表，列，的时候搜使用小写字母 + 下划线。

禁止开头出现出自。



关键字无所谓，但是mysql定义里面说是最好大些写。

写法上，每个关键字单独一行。



通常

Insert into 表名

​      Values 

#### 函数

create function function_name(

​     in in_name1 varchar(5)

​     out out_name1)

returns integer

begin

​     if ()

​             begin

​               End

End



set该已经有的变量赋值，declare给还没有的变量赋值。

#### Archer 和char

一个byte表示256个状态。

char（3）定长，每个都占用3个字节。

而对于varchar，只是占用刚好够用的字节加上一个len表示长度。

- char对于英文每个一个字节，汉子两个字节
- varchar：每个英文和汉子都是两个字节

#### isnot

is not不能用于null

[截屏2020-06-18 下午10.53.08](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-06-18 下午10.53.08.png)

#### >, >all

》all，〉some。

unique（）（不存在重复分组）

exists（分组不为空）。

#### Date

使用datediff比较日期



#### round

round限制浮点数的输出范围

而avg求平均对于Enum枚举数来说，作用很有意思。

#### if

if（， ， ）

如果条件正确输出左边，否则输出右边

#### count(*)

- myisam把总行数存在磁盘，直接返回
- innodb一行一行读出来，比较麻烦。（通过MVCC多版本控制，没一行都要判断是否对于本会话可见）（由于辅助索引树会比较小，因此会优先遍历辅助索引）因此count（*）要比count（主键id）更快

https://www.jianshu.com/p/b98ea90e6730

#### 外键

另一张表的主键

#### 大量数据如何分页显示

- 使用limit

- 使用一个页面标示page，先查询所有的page id。

  服务器通过页号和query id数组算出待查询的query id的最大和最小

![截屏2020-06-20 下午3.57.44](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-06-20 下午3.57.44.png)

#### 读写分离

每个新机器逐渐从写变成只提供读的机器。

确保写服务器压力小，而读服务器又延迟小。

#### 数据库连接池

保存数据库连接，用来给后面使用。

避免连接的频繁建立和创建。

- maxLifeTime：连接最大的使用时间，过期就会被超活
- maxOPEN:最大可打开连接。

本质上多连接只不过是让cpu分时间片服务

数量不要太多，本质上，太多分片服务慢。

也不要刚好等于核心数，这样在io阻塞的时候，还能使用cpu。

 （核心数 * 2 + 有效磁盘数（数据没有被缓存的磁盘数））

https://www.jianshu.com/p/472ff0694567

- maxIdle：最多的空闲连接。：空闲连接太多占用内存。

而且会自动变得不可用，mysql侧会自动关闭八个小时没有使用的连接。

https://blog.csdn.net/qq_39384184/article/details/103954821

#### PREPARE

普通sql语句

- 转换sql
- 编译sql
- 优化数据查询
- 执行最优查询

而perpare会做前三部，先把语句送到数据库预处理然后缓存，之后exec可以重用。

#### 临时表

如下几种情况会创建临时表

临时表，顾名思义是因为要有个临时的表来处理数据

- union查询
- group by和order by子句不一样
- from中的子查询
- 子查询
- order by中加上distinct子句
- 从同一表中select再插入同一个表，insert 。。。select

临时表过大，则会创建内存临时表



#### 优化

- 只取出要取的数据
- 使用索引
- 使用join替代子查询：子查询会使用临时表
- 小表驱动大表：in和exists的区别https://blog.csdn.net/codejas/article/details/78632883小表join大表
- 使用事务确保操作完整性



表

- 尽量not null
- 尽量固定长度

#### 缓存

#### 缓存穿透：

请求缓存里面没有数据；比如黑客拿一个不存在的数据id查询。

              -  解决方案，如果查询数据库也为空，直接设置一个默认值放到缓存中，然后设置一个过期时间
              -  过滤和系统无关的查找
              -  使用布隆过滤器



#### 缓存击穿

- 体验体验大量请求到一个key，然而这个key刚好失效，就会导致大量请求都打到数据库上
- 让第一个线程拿锁，然后做缓存。

其余的等待然后发现有缓存。



#### 缓存雪崩

某一时刻大量数据失效，导致cpu挂掉

- 让缓存失效时间增加一个随机值，避免同一时间压力太大
- 设置一个最大队列，队列有最大长度



#### 一致性hash算法

- 普通hash算法， 比如hash%n，数据和结点大小动态绑定



使用32位的hash函数，对机器的id做一个计算。

以及缓存的key值。

然后顺时针距离最近的key就存放于对应的机器上。

一般机器一个不够，因此会有虚拟结点的概念，一个机器对应50个左右的虚拟结点，使得更为均匀。





#### 去中心缓存groupcache原理。

cache分为两部分

- maincache：缓存本地机器上的缓存
- hotcache：热点数据，缓存其他机器上的缓存



获取流程

- 从本地maincache获取，从hotcache中获取
- 然后从其他结点中获取

如果很多线程，并发访问同一个value，而且缓存都没有cache到。

那么就会运行load函数（向其他机器-》从本地获取）（使用singleflight串行化）

- singleFlight（确保多个线程对同一个key能够线性化）

通过一个map保存着key和对应的call（一个请求）和一个mutex。

所有协程先加锁，然后竞争成功的生成call，然后使用waitgroup计数增一。

然后这个程序结束的时候把获得的值放进call里头去。

然后其他线程如果在map里头发现了对应的key以及call，那么就等待当前线程结束。

然后从call中取出对应的值。

#### load 返回一个value，表示从cache中取得的值，然后放到结构体里头

- 还得在运行一边lookupcache函数。防止由于goroutine的调度而错过。

- 然后用一致性hash算法选择合适的机器，从其他机器中获取，然后以1 / 10的概率存储populate（hotcache）中。

- 访问本地数据库，populate（mainCache）
- getlocally不仅会把对应的值放进cache里头，同样也会存到返回的结构体内。

（这就是getlocally的设置，仅仅是给数据库一个key，然后它把value放到一个结构体里头）。

因此我们又个populated标示为，如果调用了getlocall就是true。

这样子是为了，如果线程对一个结构体，getlocally。那么当它完成load的时候就没有必要再存储一遍值了，因为涉及到marshal啥的。

- populate会先把值存进对应的数据库；但是如果maincache + hotcache太大了的话；就会从mainCache中移除旧值；如果hotCache大于mainCache的八分之一的话，那么就从hotcache中移除。

- 而所谓的cache用一个读写锁维护读写一致性以及一个lrucache。

  



lrucache使用一个map做[key]value的存储，然后用一个list双向链表记录哪个key在前面。



- http那边以group 和 keu为名像对应的机器发送get请求
- 使用http.Defaulttransport以及roundTrip和bytes.buffer。
- http如果收到请求的话，就会使用get从groupcache中获取。
- 那么http获取其他节点的名字只能外部给它设置好了。
- http之间以proto序列化数据



#### 自增主键

- simple insert：插入前确定行数：
- bulk insert：插入前不能确定
- mixed-insert：有一部分是确定的，以不等是增长的。



- 传统模式，一个insert获取一个表锁，然后等待插入结束
- 连续模式：对于simple insert使用互斥量，而bulk insert使用表锁

会有一个逐步申请的过程，已经插入N行，会申请 n - 1行。

比如插入8的时候，会提前申请多个id。（提前申请，只会将分配锁的过程卡住）。

- 交叉模式：对所有insert 都使用互斥量，没有表锁。但是同一条语句内可能会有空洞。



####空洞

- 0， 1， 2模式下： 如果事务回滚，那么获得的自增量会丢失
- 在大块插入下，如果在连续模式，不会有空洞；1在大块扎入时可能有空洞，因因为可能会分配过多的id;而

https://www.cnblogs.com/zhoujinyi/p/3433823.html

https://www.jianshu.com/p/cca59b515e20



- 不要使用insert select这种表锁

#### snowflake

雪花算法。

1bit不用。41bit表示时间戳， 2^41 - 1表示毫秒级别。

对于64位的id， 10bit用于表示机器，12bit表示同一ms的最大id



生成id的时候用mute加锁互斥。

然后得到当前时间到过去某个epoch的时间值。

http://www.machengyu.net/tech/2019/12/04/snowflake.html

#### undo log

undo提供了对数据的回滚功能。



#### redo log

redo日志提供重做日志