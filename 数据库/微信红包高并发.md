#### 微信红包

#### 高并发

- 微信红包本质是小额资金在用户帐户流转，有发、抢、拆三大步骤。在这个过程中对事务有高要求，所以



[![微信扫一扫关注！](http://www.52im.net/template/qu_115style/img/52im_qr_4tiezi3.png)](http://www.52im.net/thread-2792-1-1.html)本文来自微信团队工程师方乐明的技术分享，由InfoQ编辑发布，即时通讯网收录时有修订和改动。原文地址：infoq.cn/article/2017hongbao-weixin，感谢原作者的分享。 一、引言 每年节假日，微信红包的收发数量都会暴涨，尤以除夕为最。如此大规模、高峰值的业务需要，背后需要怎样的技术支撑？百亿级别的红包规模，如何保证并发性能与资金安全？  本文将为读者介绍微信百亿级别红包背后的高并发设计实践，内容包括微信红包系统的技术难点、解决高并发问题通常使用的方案，以及微信红包系统的所采用高并发解决方案。 二、分享者 **方乐明：**现任微信支付应用产品系统负责人，主要从事微信红包、微信转账、微信群收款等支付应用产品的系统设计、可用性提升、高性能解决方案设计等，曾连续多年负责春节微信红包系统的性能优化与稳定性提升，取得良好的效果。 三、系列文章 **❶ 系列文章目录：**  《[社交软件红包技术解密(一)：全面解密QQ红包技术方案——架构、技术实现等](http://www.52im.net/thread-2202-1-1.html)》《[社交软件红包技术解密(二)：解密微信摇一摇红包从0到1的技术演进](http://www.52im.net/thread-2519-1-1.html)》《[社交软件红包技术解密(三)：微信摇一摇红包雨背后的技术细节](http://www.52im.net/thread-2533-1-1.html)》《[社交软件红包技术解密(四)：微信红包系统是如何应对高并发的](http://www.52im.net/thread-2548-1-1.html)》（* 本文）《[社交软件红包技术解密(五)：微信红包系统是如何实现高可用性的](http://www.52im.net/thread-2564-1-1.html)》《[社交软件红包技术解密(六)：微信红包系统的存储层架构演进实践](http://www.52im.net/thread-2568-1-1.html)》《[社交软件红包技术解密(七)：支付宝红包的海量高并发技术实践](http://www.52im.net/thread-2573-1-1.html)》《[社交软件红包技术解密(八)：全面解密微博红包技术方案](http://www.52im.net/thread-2576-1-1.html)》《[社交软件红包技术解密(九)：谈谈手Q春节红包的设计、容灾、运维、架构等](http://www.52im.net/thread-2583-1-1.html)》《[社交软件红包技术解密(十)：手Q客户端针对2020年春节红包的技术实践](http://www.52im.net/thread-2966-1-1.html)》  **❷ 其它相关文章：**  《[技术往事：“QQ群”和“微信红包”是怎么来的？](http://www.52im.net/thread-874-1-1.html)》《[QQ 18年：解密8亿月活的QQ后台服务接口隔离技术](http://www.52im.net/thread-1093-1-1.html)》《[月活8.89亿的超级IM微信是如何进行Android端兼容测试的](http://www.52im.net/thread-1086-1-1.html)》《[开源libco库：单机千万连接、支撑微信8亿用户的后台框架基石 [源码下载\]](http://www.52im.net/thread-623-1-1.html)》《[微信技术总监谈架构：微信之道——大道至简(演讲全文)](http://www.52im.net/thread-200-1-1.html)》《[微信技术总监谈架构：微信之道——大道至简(PPT讲稿) [附件下载\]](http://www.52im.net/thread-199-1-1.html)》《[如何解读《微信技术总监谈架构：微信之道——大道至简》](http://www.52im.net/thread-201-1-1.html)》《[微信海量用户背后的后台系统存储架构(视频+PPT) [附件下载\]](http://www.52im.net/thread-186-1-1.html)》《[微信异步化改造实践：8亿月活、单机千万连接背后的后台解决方案](http://www.52im.net/thread-624-1-1.html)》《[微信朋友圈海量技术之道PPT [附件下载\]](http://www.52im.net/thread-178-1-1.html)》《[架构之道：3个程序员成就微信朋友圈日均10亿发布量[有视频\]](http://www.52im.net/thread-177-1-1.html)》《[快速裂变：见证微信强大后台架构从0到1的演进历程（一）](http://www.52im.net/thread-168-1-1.html)》《[快速裂变：见证微信强大后台架构从0到1的演进历程（二）](http://www.52im.net/thread-170-1-1.html)》《[微信“红包照片”背后的技术难题](http://www.52im.net/thread-128-1-1.html)》《[微信技术分享：微信的海量IM聊天消息序列号生成实践（算法原理篇）](http://www.52im.net/thread-1998-1-1.html)》《[微信技术分享：微信的海量IM聊天消息序列号生成实践（容灾方案篇）](http://www.52im.net/thread-1999-1-1.html)》  四、微信红包的两大业务特点 微信红包（尤其是发在微信群里的红包，即群红包），业务形态上很类似网上的普通商品“秒杀”活动。  **就像下面这样：**  1）用户在微信群里发一个红包，等同于是普通商品“秒杀”活动的商品上架；2）微信群里的所有用户抢红包的动作，等同于“秒杀”活动中的查询库存；3）用户抢到红包后拆红包的动作，则对应“秒杀”活动中用户的“秒杀”动作。  不过除了上面的相同点之外，微信红包在业务形态上与普通商品“秒杀”活动相比，还具备自身的特点。  ***首先：微信红包业务比普通商品“秒杀”有更海量的并发要求。\***  微信红包用户在微信群里发一个红包，等同于在网上发布一次商品“秒杀”活动。假设同一时间有 10 万个群里的用户同时在发红包，那就相当于同一时间有 10 万个“秒杀”活动发布出去。10 万个微信群里的用户同时抢红包，将产生海量的并发请求。  ***其次：微信红包业务要求更严格的安全级别。***  微信红包业务本质上是资金交易。微信红包是微信支付的一个商户，提供资金流转服务。  用户发红包时，相当于在微信红包这个商户上使用微信支付购买一笔“钱”，并且收货地址是微信群。当用户支付成功后，红包“发货”到微信群里，群里的用户拆开红包后，微信红包提供了将“钱”转入折红包用户微信零钱的服务。  资金交易业务比普通商品“秒杀”活动有更高的安全级别要求。普通的商品“秒杀”商品由商户提供，库存是商户预设的，“秒杀”时可以允许存在“超卖”（即实际被抢的商品数量比计划的库存多）、“少卖”（即实际被抢的商户数量比计划的库存少）的情况。但是对于微信红包，用户发 100 元的红包绝对不可以被拆出 101 元；用户发 100 元只被领取 99 元时，剩下的 1 元在 24 小时过期后要精确地退还给发红包用户，不能多也不能少。  以上是微信红包业务模型上的两大特点。

**般来说，对 DB 的操作流程有以下三步：**



- 1）锁库存；
- 2）插入“秒杀”记录；
- 3）更新库存。


其中，锁库存是为了避免并发请求时出现“超卖”情况。同时要求这三步操作需要在一个事务中完成（所谓的事务，是指作为单个逻辑工作单元执行的一系列操作，要么完全地执行，要么完全地不执行）。

“秒杀”系统的设计难点就在这个事务操作上。商品库存在 DB 中记为一行，大量用户同时“秒杀”同一商品时，第一个到达 DB 的请求锁住了这行库存记录。在第一个事务完成提交之前这个锁一直被第一个请求占用，后面的所有请求需要排队等待。同时参与“秒杀”的用户越多，并发进 DB 的请求越多，请求排队越严重。因此，并发请求抢锁，是典型的商品“秒杀”系统的设计难点。

微信红包业务相比普通商品“秒杀”活动，具有海量并发、高安全级别要求的特点。



注意这里指的是同一个红包。。

![截屏2020-07-25 下午3.47.25](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-07-25 下午3.47.25.png)



#### 单机请求排队

![截屏2020-07-25 下午3.51.03](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-07-25 下午3.51.03.png)

通过hash，将相同的id搞到同一个server机器上。

然后实现singlefilght。

在DB的接入机dao中，搭建本机内存cache。以红包订单号为key，对同一个红包的拆请求做原子计数，控制同一时刻能进DB中拆红包的并发请求数。

![截屏2020-07-25 下午3.54.45](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-07-25 下午3.54.45.png)

#### 冷热分离

红包系统的分库表规则，初期是根据红包 ID 的 hash 值分为多库多表。随着红包数据量逐渐增大，单表数据量也逐渐增加。而 DB 的性能与单表数据量有一定相关性。当单表数据量达到一定程度时，DB 性能会有大幅度下降，影响系统性能稳定性。采用冷热分离，将历史冷数据与当前热数据分开存储，可以解决这个问题。

处理微信红包数据的冷热分离时，系统在以红包 ID 维度分库表的基础上，增加了以循环天分表的维度，形成了双维度分库表的特色。

具体来说，就是分库表规则像 db_xx.t_y_dd 设计，其中，xx/y 是红包 ID 的 hash 值后三位，dd 的取值范围在 01~31，代表一个月天数最多 31 天。

通过这种双维度分库表方式，解决了 DB 单表数据量膨胀导致性能下降的问题，保障了系统性能的稳定性。同时，在热冷分离的问题上，又使得数据搬迁变得简单而优雅。



订单hash纬度，是为了将订单打散到不同的DB服务器中，均衡压力。订单日期循环日纬度，是为了避免单表数据无限扩张，使每天都是一张空表。

另外，红包的订单访问热度，是非常典型的冷热型。热数据集中在一两天内，且随时间急剧消减。线上热数据库只需要存几天的数据，其他数据可以定时移到成本低的冷数据库中。循环日表也使得历史数据的迁移变得方便。

#### cdn服务器

在南北，搭建两个不同的集群。

请求法网最近的服务器。



发拆都需要使用db

#### 预订单

支付前订单落cache，同时利用cache的原子incr操作顺序生成红包订单号。优点是cache的轻量操作，以及减少DB废单。在用户请求发红包与真正支付之间，存在一定的转化率，部分用户请求发红包后，并不会真正去付款。

#### 拆红包入栈异步化

信息流与资金流分离。拆红包时，DB中记下拆红包凭证，然后异步队列请求入账。入账失败通过补偿队列补偿，最终通过红包凭证与用户账户入账流水对账，保证最终一致性。



#### cache

1、Cache住所有查询，两层cache

除了使用ckv做全量缓存，还在数据访问层dao中增加本机内存cache做二级缓存，cache住所有读请求。

查询失败或者查询不存在时，降级内存cache；内存cache查询失败或记录不存在时降级DB。

DB本身不做读写分离。



#### 流量控制

因为抢红包时，只需要查cache中的数据，不需要请求DB。对于红包已经领完、用户已经领过、红包已经过期等流量可以直接拦截。而对于有资格进入拆红包的请求量，也做流量控制。通过这些处理，最后可进入拆环节的流量大大减少，并且都是有效请求。

#### db简化

1) 订单表只存关键字段，其他字段只在cache中存储，可柔性。

红包详情的展示中，除了订单关键信息（用户、单号、金额、时间、状态）外，还有用户头像、昵称、祝福语等字段。这些字段对交易来说不是关键信息，却占据大量的存储空间。

将这些非关键信息拆出来，只存在cache，用户查询展示，而订单中不落地。这样可以维持订单的轻量高效，同时cache不命中时，又可从实时接口中查询补偿，达到优化订单DB容量的效果。

https://blog.csdn.net/varyall/article/details/78658427



#### 领取金额的计算

最少一份，最高两百。



然后，计算出本轮红包最少要领取多少，才能保证红包领完，即本轮下水位；轮最多领取多少，才能保证每个人都领到，即本轮上水位。主要方式如下：

计算本轮红包金额下水位：假设本轮领到最小值1分，那接下来每次都领到200元红包能领完，那下水位为1分；如果不能领完，那按接下来每次都领200元，剩下的本轮应全部领走，是本轮的下水位。

计算本轮红包上水位：假设本轮领200元，剩下的钱还足够接下来每轮领1分钱，那本轮上水位为200元；如果已经不够领，那按接下来每轮领1分，计算本轮的上水位。



为了使红包金额不要太悬殊，使用红包均值调整上水位。如果上水位金额大于两倍红包均值，那么使用两倍红包均值作为上水位。换句话说，每一轮抢到的红包金额，最高为两倍剩下红包的均值。

最后，获取随机数并用上水位取余，（记住取余）如果结果比下水位还小，则直接使用下水位，否则使用随机金额为本轮拆到金额。



#### 手机qq移动端


***1）资源预加载：\***

QQ 红包中用到的不经常变化的静态资源，如页面，图片，JS 等，会分发到各地 CDN 以提高访问速度，只有动态变化的内容，才实时从后台拉取。然而即使所有的静态资源都采用了 CDN 分发，如果按实际流量评估，CDN 的压力仍然无法绝对削峰。因为同时访问红包页面的人数比较多，按 83 万 / 秒的峰值，一个页面按 200K 评估，约需要 158.3G 的 CDN 带宽，会给 CDN 带来瞬间很大的压力。为减轻 CDN 压力，QQ 红包使用了手机 QQ 离线包机制提前把红包相关静态资源预加载到手机QQ移动端，这样可大大降低 CDN 压力。

目前手机 QQ 离线包有两种预加载方式：



- a. 将静态资源放入预加载列表：用户重新登录手机 QQ 时监测离线包是否有更新并按需加载（1 天能覆盖 60%，2 天能覆盖 80%，适合预热放量情况）；
- b. 主动推送离线包：向当前在线用户推送离线包。（2 个小时可以完成推送，覆盖总量的 40% 左右，适合紧急情况）通过离线包预加载后，除夕当天的 CDN 流量并没有出现异常峰值，比较平稳。

***2）缓存和延时：\***
2.59 亿用户同时在线，用户刷一刷时的峰值高达 83 万 / 秒，如果这些用户的操作请求全部同时拥向后台，即使后台能抗得住，需要的带宽、设备资源成本也是天文数字。为了尽可能减轻后台服务器压力，根据用户刷一刷的体验，用户每次刷的操作都向后台发起请求是没有必要的，因此手机 QQ 在移动端对用户刷一刷的操作进行计数，定时（1~3 秒）异步将汇总数据提交到后台抽奖，再将抽奖结果回传到手机 QQ 移动端显示。这样既保证了“刷”的畅快体验，也大大减轻后台压力，抽奖结果也在不经意间生产，用户体验完全无损。

***通过对活动入口随机时间错峰显示，控制对抽奖后台的请求：\***

将所有用户随机均匀地映射到活动开始后的一段时间区间内，使用户错峰显示入口进入参与活动，如2019年春节的福袋。错峰时间的算法形如：hash(uin) % gap。

![截屏2020-07-25 下午4.27.19](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-07-25 下午4.27.19.png)

#### 

### 字体提前绘制


个性化红包支持所有简体与繁体汉字，并支持部分简体汉字转换成繁体汉字，为了改善使用“姓氏红包”用户的体验，我们把常用的 300 个姓氏，使用预生成的方式，在用户手机 QQ 空闲的时候生成常用的姓氏图片保存到本地。其他的非常用姓氏，在展示的时候合成，合成一次保存在本地，下次在本地读取。

手机 QQ 移动端在空闲时绘制好字体贴图，支持定时更新背景图和字体库，对非常用字，则启动个性化字体引擎生成对应的个性化贴图。

http://www.52im.net/thread-2202-1-1.html



http://www.52im.net/thread-2966-1-1.html





#### 库存

业界最为常见的是预扣库存。无论是外卖点餐还是电商购物，下单后一般都有个 “有效付款时间”，超过该时间订单自动释放，这就是典型的预扣库存方案。但如上所述，预扣库存还需要解决恶意下单的问题，保证商品卖的出去；另一方面，如何避免超卖，也是一个痛点。

1. 卖的出去：恶意下单的解决方案主要还是结合安全和反作弊措施来制止。比如，识别频繁下单不付款的买家并进行打标，这样可以在打标买家下单时不减库存；再比如为大促商品设置单人最大购买件数，一人最多只能买 N 件商品；又或者对重复下单不付款的行为进行次数限制阻断等
2. 避免超卖：库存超卖的情况实际分为两种。对于普通商品，秒杀只是一种大促手段，即使库存超卖，商家也可以通过补货来解决；而对于一些商品，秒杀作为一种营销手段，完全不允许库存为负，也就是在数据一致性上，需要保证大并发请求时数据库中的库存字段值不能为负，一般有多种方案：一是在通过事务来判断，即保证减后库存不能为负，否则就回滚；二是直接设置数据库字段类型为无符号整数，这样一旦库存为负就会在执行 SQL 时报错；三是使用 CASE WHEN 判断语句——

作者：阿哲
        链接：https://segmentfault.com/a/1190000020970562
        来源：SegmentFault 思否
        著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。