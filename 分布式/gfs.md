#### GFS

- 节点失效将被看成是正常情况，而不再视为异常情况。整个文件系统包含了几百个或者几千个由廉价的普通机器组成的存储机器，而且这些机器是被与之匹配数量的客户端机器访问。这些节点的质量和数量都实际上都确定了在任意给定时间上，一定有一些会处于失效状态，并且某一些并不会从当前失效中恢复回来。
- ，按照传统标准来看，文件都是非常巨大的。数个GB的文件是常事。

- 大部分文件都是只会在文件尾新增加数据，而少见修改已有数据的。对一个文件的随机写操作在实际上几乎是不存在的。当一旦写完，文件就是只读的，并且一般都是顺序读取得。
- 控制流和数据流分离
- 超大chunk

#### 架构

GFS集群由一个单个的master和好多个chunkserver（块服务器）组成。

- 在GFS下，每一个文件都拆成固定大小的chunk(块)。每一个块都由master根据块创建的时间产生一个全局唯一的以后不会改变的64位的chunk handle标志。chunkservers在本地磁盘上用Linux文件系统保存这些块，并且根据chunk handle和字节区间，通过LInux文件系统读写这些块的数据。（块大小64m）.

- 客户端或者chunkserver都不会cache文件数据。客户端cache机制没啥用处，这是因为大部分的应用都是流式访问超大文件或者操作的数据集太大而不能被chache。不设计cache系统使得客户端以及整个系统都大大简化了（少了cache的同步机制）

- (不过客户端cache元数据)。
- chunkserver不需要cache文件数据，因为chunks就像本地文件一样的被保存，所以LInux的buffer cache已经把常用的数据cache到了内存里。
- master负责管理所有的文件系统的元数据。文件和chunk的namespace，文件到chunks的映射关系，每一个chunk的副本的位置。所有的元数据都是保存在master的内存里的。头两个类型（namepspaces和文件到chunk的映射）同时也是由在master本地硬盘的记录所有变化信息的operation log来持久化保存的，这个记录也会在远端机器上保存副本
- master并不持久化保存chunk位置信息。相反，他在启动地时候以及chunkserver加入集群的时候，向每一个chunkserver询问他的chunk信息。我们最开始尝试想把chunk位置信息持久化保存在master上，但是我们后来发现如果再启动时候，以及定期性从chunkserver上读取chunk位置信息会使得设计简化很多。因为这样可以消除master和chunkserver之间进行chunk信息的同步问题，当chunkserver加入和离开集群，更改名字，失效，重新启动等等时候，如果master上要求保存chunk信息，那么就会存在信息同步的问题。在一个数百台机器的组成的集群中，这样的发生chunserver的变动实在是太平常了。
- 此外，不在master上保存chunk位置信息的一个重要原因是因为只有chunkserver对于chunk到底在不在自己机器上有着最后的话语权。另外，在master上保存这个信息也是没有必要的，因为有很多原因可以导致chunserver可能忽然就丢失了这个chunk（比如磁盘坏掉了等等），或者chunkserver忽然改了名字，那么master上保存这个资料啥用处也没有。
- master也同样控制系统级别的活动，比如chunk的分配管理，孤点chunk的垃圾回收机制，chunkserver之间的chunk镜像管理。master和这些chunkserver之间会有定期的心跳线进行通讯，并且心跳线传递信息和chunckserver的状态。

#### 读取

![截屏2020-07-23 下午1.59.14](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-07-23 下午1.59.14.png)



- 首先，客户端把应用要读取的文件名和偏移量，根据固定的chunk大小，转换成为文件的chunk index。
- 然后向master发送这个包含了文件名和chunkindex的请求。master返回相关的chunk handle以及对应的位置。客户端cache这些信息，把文件名和chunkindex作为cache的关键索引字。
  于是这个客户端就像对应的位置的chunkserver发起请求，通常这个会是离这个客户端最近的一个。请求给定了chunk handle以及一个在这个chunk内需要读取得字节区间。在这个chunk内，再次操作数据将不用再通过客户端-master的交互，除非这个客户端本身的cache信息过期了，或者这个文件重新打开了。

#### 超大chunk

- 首先，它减少了客户端和master的交互，因为在同一个chunk内的读写操作之需要客户端初始询问一次master关于chunk位置信息就可以了。这个减少访问量对于我们的系统来说是很显著的，因为我们的应用大部分是顺序读写超大文件的。即使是对小范围的随机读，客户端可以很容易cache一个好几个TB数据文件的所有的位置信息。
- 其次，由于是使用一个大的chunk，客户端可以在一个chunk上完成更多的操作，它可以通过维持一个到chunkserver的TCP长连接来减少网络管理量。
- 第三，它减少了元数据在master上的大小。这个使得我们可以把元数据保存在内存，这样带来一些其他的好处，详细请见2.6.1节。

（master为每64Mchunk分配的空间不到64个字节的元数据。）

- 只有两三个chunkserver保存这个可执行的文件，但是有好几百台机器一起请求加载这个文件导致系统局部过载。我们通过把这样的执行文件保存份数增加，



#### 写

![截屏2020-07-23 下午2.07.43](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-07-23 下午2.07.43.png)

1. 客户端向master请求当前chunk的令牌位置以及其他所有副本的位置。如果没有chunkserver持有这个chunk的令牌，则master选择一个chunk副本授权一个令牌（在图上没有标出）
2. master给出应答，包括了primary和其他副本位置（secondary）标记。客户端cache这些数据，用于以后的变动。只有当primary不能访问或者primary返回它不再持有令牌的时候，客户端才需要重新联系master。
3. 客户端把数据发布给每一个副本。客户端可以以任意顺序发布这些数据。每一个chunkserver都在内部的LRU缓冲中cache这些数据，这些数据一旦被提交或者过期就会从缓冲中去掉。通过把数据流和控制流的分离，我们可以不考虑哪个chunkserver是primary，通过仔细调度基于网络传输的代价昂贵的数据流，优化整体的性能。3.2节进一步讨论了这个。
4. 当所有的副本都确认收到了数据，客户端发起一个写请求给primary。这个请求标记了早先发给所有副本的数据。primary分配一系列连续的序列号给所有的收到的变动请求，这个可能是从好多客户端收到的，这提供了必要的序列化。primary按照这个序列号顺序变动他自身本地的状态。
5. prmary把写请求发布到所有的secondary副本。每一个secondary副本都依照和primary分配的相同的序列号顺序来进行变动的提交。
6. secondary副本全部都给primary应答，表示他们都已经完成了这个操作。
7. primary应答给客户端。如果有任何副本报告了任何错误，都需要报告给客户端。在发生错的情况下，写入者会在primary成功但是在secondary副本的某些机器上失败。（如果在primary失败，不会产生一个写入的序列号并且发布序列号）。客户端请求就是由失败的情况，并且修改的区域就有不一致的状态。我们的客户端代码是通过重试改动来处理这些错误。他可能会在从头开始重试前，在第三步到第7步尝试好几次。
   如果应用的一个写入操作不止一个chunk或者是跨chunk的操作。GFS客户端代码把这个写入操作分解成为多个写入操作。每一个写操作都按照上边描述的控制流进行的，这些写操作可能和其他客户端的操作并发进行交叉存取和改写。因此，虽然因为每个操作都是对每个副本相同顺序完成的，对每一个副本都是一致的，但是大家共享操作的文件区块可能最后会包含不同客户端的小块。这就使得文件区虽然一致，但是是不确定的（如同2.7节讲述的一样）。



#### 一致性

- 元数据通过master顺序化
- 而对于写入操作，我们分为consistent和defined
- ![截屏2020-07-23 下午2.40.52](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-07-23 下午2.40.52.png)

当所有的客户端都看到的是相同的数据的时候，并且与这些客户端从哪个数据的副本读取无关的时候，一个文件区是一致性的。

而defined代表内容不确定。



首先如果失败了的话，不同chunkserver的数据不是一样的。



#### 为什么并发写不是defined的

- 
- ![截屏2020-07-23 下午2.59.18](/Users/jieyang/Library/Application Support/typora-user-images/截屏2020-07-23 下午2.59.18.png)
- write大小超过一个块，会分成多个块上的操作

我们举例2个例子来说明一下。
例子1，文件目前有2个chunk，分别是chunk1, chunk2。
Client1要在54MB的位置写入20MB数据。这写入跨越了边界，要分解成2个操作，第一个操作写入chunk1最后10MB，第二个操作写入chunk2的开头10MB。
Client2也要在54MB的位置写入20MB的数据。这个写入也跨越边界，也要分解为2个操作，作为第三个操作写入chunk1最后10MB，作为第四个操作写入chunk2的开头10MB。

2个客户端并发写入数据，那么第一个操作和第三个操作在chunk1上就是并发执行的，第二个操作和第四个操作在chunk2上并发执行，如果chunk1的先执行第一操作再执行第三个操作，chunk2先执行第四个操作再执行第二个操作，那么最后，在chunk1上会保留client1的写入的数据，在chunk2上保留了client2的写入的数据。虽然client1和client2的写入都成功了，但最后既不是client1想要的结果，也不是client2想要的结果。最后的结果是client1和client2写入的混合。对于client1和client2来说，他们操作都不是原子的。

- append不会划分多个块

primary检查看看是否这些对当前chunk的增加是否会导致chunk超过最大大小（64M）。如果超过了，它就把chunk填写到最大大小，告诉secondary也跟着填写到最大，并且返回给客户端表示这个操作需要在下一个chunk重试。（纪录增加操作严格限制在1/4最大chunk大小，来保证最坏的分段情况下还是可以接受的）。



为什么record append是不一致的。

recordappend如果第一次失败，之后重试，这个不一致性会永远保留。

Record Append留下的不一致是永久的不一致，并且还会有重复问题，如果一条记录在一部分副本上成功，在另外一部分副本上失败，那么这次Record Append就会报告给客户端失败，并且让客户端重试，如果重试后成功，那么在某些副本上，这条记录就会成功的写入2次。
我们可以得出，Record Append保证是至少一次的原子操作（at least once atomic）。
————————————————
版权声明：本文为CSDN博主「cadem」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/cadem/java/article/details/102546103



#### 元数据通过锁实现一致性

每一个master操作在执行前都要求一组锁的集合。通常，如果它包含/d1/d2/…/dn/leaf，它会要求在/d1,/d1/d2,…/d1/d2/…/dn上的读锁，并且读锁以及写锁在全文件名的/d1/d2/…/dn/leaf。注意，leaf可以是这个操作相关的一个文件或者目录名。
我们现在通过一个例子来讲解这个锁机制如何有效工作的。假定我们在创建/home/user的快照/save/user的时候，如何防止/home/user/foo文件的创建。这个快照的操作要求读锁：/home以及/save，写锁/home/user和/save/user。文件创建操作要求读锁/home和/home/user，写锁/home/user/foo。这两个操作就会被正确序列化，因为他们尝试解决锁冲突/home/user。文件创建并不要求一个在父目录的写锁，因为这并没有一个需要保护的”目录”或者inode-like的数据结构。在名字上的读锁已经足够来保护父目录不被删除。



这种锁机制带来一个好处就是在同一个目录下允许并发改动。比如，在同一个目录下的多个文件创建可以并行执行；每一个要求一个在目录名上的读锁，并且要求在一个文件名上的写锁。在目录名上的读锁足以防止目录被删除，改名或者快照。在文件名上的写锁序列化对两次同一文件的创建操作。



并且，锁是基于一个相同的总顺序来分配的，这样放置死锁：他们是首先根据namespace树种的级别顺序，以及相同级别下根据字典顺序进行分配的。（避免死锁）

#### 创建文件操作

- 父目录读锁保证不被删除
- 根文件写锁实现串行化



#### 副本位置

- 这就保证了当整个机架都损坏的时候（或者掉线的时候），对于任意一个chunk来说，都有一些副本依旧有效（比如，由于共享资源的失效，例如网络交换机或者电源故障，导致机架不可用。
- 写操作会利用不同机架间的流量，而读操作时间机架内的聚合带宽（考虑机架内带宽比机架入口小）

#### 创建

- 1)我们希望新副本所在的chunkserver有着低于平均水平的磁盘空间利用率。随着时间的推进，chunkserver上的磁盘利用率会趋于均匀。
- (2)我们希望限制每一个chunkserver上的”最近”创建的数量。虽然创建操作本身的负载很轻，但是它却意味着随后立刻又很重的写操作，因为chunk是因为要写东西才会创建，并且在我们的写一次读多次的工作量下，他们通常完成写操作以后，他们实际上就成为只读的了。
- (3)如同上边讨论得这样，我们希望把副本跨越机架。



#### 删除

- 当应用删除了一个文件，master像记录其他变动一样，立刻记录这个删除操作。
- 不过不同于立刻回收资源，这个文件仅仅是改名称为一个隐藏的名字，并且包含了一个删除时戳。在master的常规的文件系统namespace的检查中，他会移出这些超过3天隐藏删除文件（这个3天是可以配置的）。
- 直到此时，文件依旧可以通过新的，特别的名字读取，并且可以通过改名来恢复成为未删除状态。当隐藏文件从namespace删除后，它在内存的元数据也随之删除。这个会影响他所指向的各个chunk。
- 在对chunk的namespace的常规扫描中，master识别chunk孤点（就是说，没有被任何文件所指向）并且删除这些孤点的元数据。在chunkserver和master的常规心跳消息中，每一个chunkserver都报告自己的chunk集合，并且master回复在master的元数据中已经不存在的chunk标记。chunkserver随即释放和删除这些chunk的副本。

#### 过期chunk

chunk副本可能会因为chunkserver失效期间丢失了对chunk的变动而导致过期。

- 对于每一个chunk，master保持一个chunk的版本号码来区分最新的和过期的副本。
- 无论何时master为一个chunk颁发一个令牌，他都增加chunk的版本号码并且通知最新的副本。master和这些副本在他们的持久化状态中都记录最新的版本号码。这在任何客户端被通知前发生，并且因此在开始对这个chunk写之前发生。



- 如果另一个副本当前不在线，那么他的chunk的版本号码就不会改变。当这个chunkserver重新启动，并且向master报告它的chunk以及相关版本号码的时候，master会根据版本号码来检查出这个chunkserver有一个过期的副本。
- 如果master发现一个更高版本号码的chunk，master会认为他在颁布令牌的时候失败了，于是会取较高的版本号来作为最新的chunk。



#### 影子chunk

进一步说，master的”影子进程”，提供了对文件系统的只读操作，即使当当前的master失效的时候也是只读的。他们是影子进程，并非镜像进程，他们可能会比primary master稍慢一拍，通常是不到一秒钟。这些进程增强了对于那些并不是很活跃修改的文件的读取能力，或者对那些读取脏数据也无所谓的应用来说，提高了读取性能。实际上，因为文件内容是从chunkserver上读取的，应永不不能发现文件内容的过期。什么原因会导致在一个很小的时间窗内文件的元数据会过期呢，目录内容或者访问控制信息的变动会导致小时间窗内元数据过期。



#### 完整数据

- 我们可以通过别的chunk副本来解决这个问题，但是如果跨越chunkserver比较这个chunk的内容来决定是否损坏就很不实际。
- 进一步说，允许不同副本的存在;在GFS更改操作的语义上，特别是早先讨论过的原子纪录增加的情况下，并不保证byte级别的副本相同。
- 因此，每一个chunkserver上必须独立的效验自己的副本的完整性，并且自己管理checksum。



- 我们把一个chunk分成64k的块。每一个都有相对应的32位的checksum。就像其他的元数据一样，checksum是在内存中保存的，并且通过分别记录用户数据来持久化保存。
  对于读操作来说，在给客户端或者chunkserver读者返回数据之前，chunkserver效验要被读取的数据所在块的checksum。
- 因此chunkserver不会把错误带给其他设备。如果一个块的checksum不正确，chunkserver会给请求者一个错误，并且告知master这个错误。收到这个应答之后，请求者应当从其他副本读取这个数据，master也会安排从其他副本来做克隆。当一个新的副本就绪后，master会指挥刚才报错的chunkserver删掉它刚才错误的副本



对写操作的影响：

- checksum的计算是针对添加到chunk尾部的写入操作作了高强度的优化（和改写现有数据不同），因为它们显然占据主要工作任务。我们增量更新关于最后block的checksum部分，并且计算添加操作导致的新的checksum block部分。即使是某一个checksum块已经损坏了，但是我们在写得时候并不立刻检查，新的checksum值也不会和已有数据吻合，下次对这个block的读取的时候，会检查出这个损坏的block。（append不用校验）

- 另一方面，如果写操作基于一个已有的chunk（而不是在最后追加），我们必须读取和效验被写得第一个和最后一个block，然后再作写操作，最后计算和写入新的checksum。如果我们不效验第一个和最后一个被写得block，那么新的checksum可能会隐藏没有改写区域的损坏部分。





#### 为什么gfs有lease机制

　回想Haystack，客户端直接将写请求提给各个Store机器即可解决问题，GFS也是对等设计，为何要捣鼓出令人费解的租赁机制？不能直接由客户端分别写入各个对等chunkserver吗？这个问题已经在一致性模型章节讨论过，当时提到了租赁机制是为了保证各个副本按相同顺序串行变异，那我们也可以反过来问一句：Haystack、TFS里为啥没有遇到这等问题？是它们对一致性考虑太少了吗？ 这个答案如果要追本溯源，还是要牵扯到编程界面的问题：Haystack、TFS中用户面对的是一个个小图片，用户将图片存进去、拿到一个ID，将来只要保证他拿着此ID依然能找到对应图片就行，即使各个副本执行存储时顺序有差别，也丝毫不影响用户使用（比如Hasytack收到3个并发图片A、B、C的存储请求，提交到3台Store机器，分别存成了ABC、ACB、BAC三种不同的顺序，导致副本内容不一致，但是当用户无论拿着A、B、C哪个图片ID来查询、无论查的是哪个Store机器，都能检索到正确的图片，没有问题；Haystack和TFS的编程界面封装的更高级，GFS在底层遭遇的难题影响不到它们）。而在GFS比较底层的编程界面中，用户面对的是众多图片组装而成的一整个文件，如果GFS在不同副本里存储的文件内容不一致，那就会影响用户的检索逻辑，那摊上大事儿了。另外，Haystack、TFS只支持小文件的append和remove，而GFS怀抱着伟大的愿景，它需要支持对文件的随机写，只有保证顺序才能避免同个文件区域并发随机写导致的undefined碎片问题。所以GFS花心思设计租赁机制也就合情合理了。



- haystack存储的是文件id，以id索引。而gfs是块的字节偏移。



#### 数据流分离

　　Haystack没法在数据流上做什么文章，它的客户端需要分别写入各个Store机器；TFS可以利用其主备机制，合理安排master-slave的拓扑位置以优化网络负载。而GFS则是完全将真实文件的数据流和控制流解耦分离，在网络拓扑、最短路径、最小化链式传输上做足了文章。系统规模到一定程度时，机房、机架网络拓扑结构对于整体性能的影响是不容忽略的，GFS在这种底层机制上考虑的确实更加到位



#### 删除

同样，GFS的删除与Haystack、TFS的删除，其意义也不同。GFS的删除指的是整个大文件的删除。Haystack和TFS删除的是用户视角下的一条数据，比如一张图片，它是逻辑存储单元中的一个entry而已。而GFS的删除则是用户视角下的一整个文件，一个文件就对应了多个逻辑存储单元（chunk），里面也包含了海量的应用实体（比如图片）。在这种差异的背景下，他们面临的难题也完全不同。Haystack和TFS面临的就是刚才提到的“删除会破坏存储文件已有数据的格式、造成狭缝碎片”问题，它们的对策就是懒惰软删除、闲了再整理。而GFS则不会遭遇此难题，用户在文件内部删除几条数据对于GFS来说和随机偏移写没啥区别，狭缝碎片也是用户自己解决。GFS需要解决的是整个文件被删除，遗留下了大量的chunk，如何回收的问题。相比Haystack和TFS，GFS的垃圾回收其实更加轻松，因为它要回收的是一个个逻辑存储单元（chunk），一个chunk（副本）其实就是一个真实的Linux文件，调用file.delete()删了就等于回收了，而不需要担心文件内部那些精细的组织格式、空间碎片。



- 随机写跨数据块难以实现
- 

https://kb.cnblogs.com/page/174130/

#### 元数据

难道GFS不会遇到这个难题吗？而且此篇文章还反复强调了GFS的master是单例的、纯内存的，难道它真的不会遭遇单点瓶颈吗？答案还是同一个：编程界面。GFS的编程界面决定了即使它整个集群存了几百TB的数据，也不会有太多的文件。对于Haystack和TFS，它们面对的是billions的图片文件，对于GFS，它可能面对的仅仅是一个超巨型文件，此文件里有billions的图片数据。也就是说Haystack和TFS要保存billions的应用元数据，而GFS只需保存一个。那GFS把工作量丢给谁了？对，丢给用户了。所以GFS虽然很伟大、很通用、愿景很酷，但是对特定应用场景的支持不一定友好，这也是Haystack最终自行定制的原因之一吧。





#### gfs和hdfs

HDFS 在考虑写入模型时做了一个简化，就是同一时刻只允许一个写入者或追加者。 
在这个模型下同一个文件同一个时刻只允许一个客户端写入或追加。 
而 GFS 则允许同一时刻多个客户端并发写入或追加同一文件。

允许并发写入带来了更复杂的一致性问题。 
多个客户端并发写入时，它们之间的顺序是无法保证的，同一个客户端连续追加成功的多个记录也可能被打断。 
这意味着一个客户端在连续写入文件数据时，它的数据最终在文件中的分布可能是不连续的。



所谓一致性就是，对同一个文件，所有的客户端看到的数据是一致的，不管它们是从哪个副本读取的。 
如果允许多个客户端同时写一个文件，怎么保证写入数据在多个副本间一致？ 
我们前面讲 HDFS 时它只允许一个写入者按流水线方式写入多个副本，写入顺序一致，写入完成后数据将保持最终一致。 
而对多个客户端而言，就必须让所有同时写入的客户端按同一种流水线方式去写入，才可能保证写入顺序一致。 
这个写入流程我们下一节详细分析。





- GFS允许文件被多次或者多个客户端同时打开以追加数据，以记录为单位。假设GFS追加记录的大小为16KB ~ 16MB之间，平均大小为1MB，如果每次追加都访问GFS Master显然很低效，因此，GFS通过Lease机制将每个Chunk的写权限授权给Chunk Server。写Lease的含义是Chunk Server对某个Chunk在Lease有效期内(假设为12s)有写权限，拥有Lease的Chunk Server称为Primary Chunk Server，如果Primary Chunk Server宕机，Lease有效期过后Chunk的写Lease可以分配给其它Chunk Server。

  　　多客户端并发追加同一个文件导致Chunk Server需要对记录进行定序，客户端的写操作失败后可能重试，从而产生重复记录，再加上客户端API为异步模型，又产生了记录乱序问题。

  　　Append模型下重复记录、乱序等问题加上Lease机制，尤其是同一个Chunk的Lease可能在Chunk Server之间迁移，极大地提高了系统设计和一致性模型的复杂度。而在HDFS中，HDFS文件只允许一次打开并追加数据，客户端先把所有数据写入本地的临时文件中，等到数据量达到一个Chunk的大小(通常为64MB)，请求HDFS Master分配工作机及Chunk编号，将一个Chunk的数据一次性写入HDFS文件。

  　　由于累积64MB数据才进行实际写HDFS系统，对HDFS Master造成的压力不大，不需要类似GFS中的将写Lease授权给工作机的机制，且没有了重复记录和乱序的问题，大大地简化了系统的设计。然而，我们必须知道，HDFS由于不支持Append模型带来的很多问题，构建于HDFS之上的Hypertable和HBase需要使用HDFS存放表格系统的操作日志，由于HDFS的客户端需要攒到64MB数据才一次性写入到HDFS中，Hypertable和HBase中的表格服务节点(对应于Bigtable中的Tablet Server)如果宕机，部分操作日志没有写入到HDFS，可能会丢数据。其次是Master单点失效的处理。

- https://blog.csdn.net/xiaofei0859/article/details/53466008



https://juejin.im/entry/5ab98afb518825555e5dda81